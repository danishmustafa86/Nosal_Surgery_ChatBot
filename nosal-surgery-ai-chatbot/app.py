import streamlit as st
import os
from openai import OpenAI
from bs4 import BeautifulSoup
import html2text
import re
from langdetect import detect
import tempfile
import base64
from audio_recorder_streamlit import audio_recorder
from dotenv import load_dotenv
import hashlib
import json
import time
from datetime import datetime
import pandas as pd
from collections import defaultdict
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import requests
from urllib.parse import urlparse, urljoin

# Load environment variables
load_dotenv()

# Page configuration
st.set_page_config(
    page_title="Meko Clinic Rhinoplasty Chatbot",
    page_icon="üè•",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Language mapping with voice support
LANGUAGES = {
    "English": {"code": "en", "native": "English", "voice": "alloy"},
    "Spanish": {"code": "es", "native": "Espa√±ol", "voice": "nova"},
    "French": {"code": "fr", "native": "Fran√ßais", "voice": "alloy"},
    "German": {"code": "de", "native": "Deutsch", "voice": "echo"},
    "Italian": {"code": "it", "native": "Italiano", "voice": "alloy"},
    "Portuguese": {"code": "pt", "native": "Portugu√™s", "voice": "nova"},
    "Russian": {"code": "ru", "native": "–†—É—Å—Å–∫–∏–π", "voice": "fable"},
    "Chinese": {"code": "zh", "native": "‰∏≠Êñá", "voice": "shimmer"},
    "Japanese": {"code": "ja", "native": "Êó•Êú¨Ë™û", "voice": "shimmer"},
    "Korean": {"code": "ko", "native": "ÌïúÍµ≠Ïñ¥", "voice": "onyx"},
    "Arabic": {"code": "ar", "native": "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©", "voice": "onyx"},
    "Hindi": {"code": "hi", "native": "‡§π‡§ø‡§Ç‡§¶‡•Ä", "voice": "nova"},
    "Urdu": {"code": "ur", "native": "ÿßÿ±ÿØŸà", "voice": "nova"},
    "Turkish": {"code": "tr", "native": "T√ºrk√ße", "voice": "echo"},
    "Thai": {"code": "th", "native": "‡πÑ‡∏ó‡∏¢", "voice": "alloy"}
}

# Language detection mapping
LANG_DETECT_MAP = {
    "en": "English", "es": "Spanish", "fr": "French", "de": "German",
    "it": "Italian", "pt": "Portuguese", "ru": "Russian", "zh": "Chinese",
    "zh-cn": "Chinese", "zh-tw": "Chinese", "ja": "Japanese", "ko": "Korean",
    "ar": "Arabic", "hi": "Hindi", "ur": "Urdu", "tr": "Turkish", "th": "Thai"
}

# Quick response templates for common questions
QUICK_RESPONSES = {
    "pricing": {
        "Thai": "üí∞ ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å‡πÅ‡∏ö‡∏ö Open ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà 99,000 ‡∏ö‡∏≤‡∏ó\n\n‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:\n- ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏™\n- ‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≤‡πÅ‡∏û‡∏ó‡∏¢‡πå ‡∏Ñ‡πà‡∏≤‡∏´‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î\n- ‡∏°‡∏µ‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏ú‡πà‡∏≠‡∏ô‡∏ä‡∏≥‡∏£‡∏∞‡πÑ‡∏î‡πâ\n\n‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤: +66 2 272 0022",
        "English": "üí∞ Open Rhinoplasty pricing starts at 99,000 THB\n\nDetails:\n- Price varies based on case complexity\n- Includes surgeon, operating room, and post-op care\n- Promotions and payment plans available\n\nContact for pricing: +66 2 272 0022"
    },
    "consultation": {
        "Thai": "üìã ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏û‡∏ó‡∏¢‡πå\n\n‚úÖ ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ü‡∏£‡∏µ‡∏ú‡πà‡∏≤‡∏ô‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå\n‚úÖ ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•\n‚úÖ ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î\n‚úÖ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î\n\n‡∏ô‡∏±‡∏î‡∏´‡∏°‡∏≤‡∏¢: +66 2 272 0022\n‡∏´‡∏£‡∏∑‡∏≠ Facebook Messenger: @MEKOCLINIC",
        "English": "üìã Consultation Process\n\n‚úÖ Free online consultation\n‚úÖ Digital facial assessment\n‚úÖ Surgical planning\n‚úÖ Pre-operative instructions\n\nBook appointment: +66 2 272 0022\nOr Facebook Messenger: @MEKOCLINIC"
    },
    "recovery": {
        "Thai": "ü©π ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏ü‡∏∑‡πâ‡∏ô\n\n‚Ä¢ 1-2 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå: ‡πÅ‡∏ú‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏´‡∏≤‡∏¢\n‚Ä¢ 2-3 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå: ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ\n‚Ä¢ 6-12 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà\n\n‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î:\n- ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Å‡∏£‡∏∞‡πÄ‡∏ó‡∏∑‡∏≠‡∏ô\n- ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥\n- ‡∏°‡∏≤‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ï‡∏≤‡∏°‡∏ô‡∏±‡∏î",
        "English": "ü©π Recovery Timeline\n\n‚Ä¢ 1-2 weeks: Initial healing\n‚Ä¢ 2-3 weeks: Return to work\n‚Ä¢ 6-12 months: Full results\n\nPost-op care:\n- Avoid trauma to nose\n- Follow cleaning instructions\n- Attend follow-up appointments"
    },
    "contact": {
        "Thai": "üìû ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡πÄ‡∏°‡πÇ‡∏Å‡∏∞‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å\n\nüì± ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£: +66 2 272 0022\nüí¨ Facebook Messenger: @MEKOCLINIC\nüìß WhatsApp: +66 2 272 0022\nüåê ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå: mekoclinic.com\n\nüìç ‡∏™‡∏≤‡∏Ç‡∏≤: ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø, ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢",
        "English": "üìû Contact Meko Clinic\n\nüì± Phone: +66 2 272 0022\nüí¨ Facebook Messenger: @MEKOCLINIC\nüìß WhatsApp: +66 2 272 0022\nüåê Website: mekoclinic.com\n\nüìç Location: Bangkok, Thailand"
    }
}

# Related links and resources database
RELATED_RESOURCES = {
    "rhinoplasty": {
        "videos": [
            {
                "title": "‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ö‡∏£‡πâ‡∏≠‡∏á‡πÇ‡∏≠‡πÇ‡∏´‡∏´ ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà ‡πÄ‡∏°‡πÇ‡∏Å‡∏∞‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å",
                "url": "https://www.youtube.com/watch?v=example1",
                "description": "‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å"
            },
            {
                "title": "‡∏Ñ‡∏∏‡∏ì‡∏û‡∏•‡∏≠‡∏¢ ‡∏û‡∏•‡∏≠‡∏¢‡∏û‡∏£‡∏£‡∏ì ‡πÄ‡∏ú‡∏¢‡∏à‡∏°‡∏π‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏™‡∏ß‡∏¢‡πÄ‡∏õ‡πä‡∏∞ ‡∏ó‡∏µ‡πà‡πÄ‡∏°‡πÇ‡∏Å‡∏∞ ‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å",
                "url": "https://www.youtube.com/watch?v=example2",
                "description": "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å Open technique"
            },
            {
                "title": "‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å Open ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏°‡∏π‡∏Å‡πÉ‡∏´‡πâ‡∏™‡πÇ‡∏•‡∏õ‡∏™‡∏ß‡∏¢",
                "url": "https://www.youtube.com/watch?v=example3",
                "description": "‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡∏¥‡∏î"
            }
        ],
        "websites": [
            {
                "title": "Meko Clinic Official Website",
                "url": "https://mekoclinic.com",
                "description": "‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡πÇ‡∏Å‡∏∞‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å"
            },
            {
                "title": "Rhinoplasty Information - Mayo Clinic",
                "url": "https://www.mayoclinic.org/tests-procedures/rhinoplasty/about/pac-20384532",
                "description": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å"
            },
            {
                "title": "American Society of Plastic Surgeons - Rhinoplasty",
                "url": "https://www.plasticsurgery.org/cosmetic-procedures/rhinoplasty",
                "description": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏™‡∏°‡∏≤‡∏Ñ‡∏°‡∏®‡∏±‡∏•‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏û‡∏•‡∏≤‡∏™‡∏ï‡∏¥‡∏Å‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏±‡∏ô"
            }
        ]
    },
    "recovery": {
        "videos": [
            {
                "title": "‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å - 7 ‡∏ß‡∏±‡∏ô‡πÅ‡∏£‡∏Å",
                "url": "https://www.youtube.com/watch?v=recovery1",
                "description": "‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î"
            }
        ],
        "websites": [
            {
                "title": "Post-Operative Care Guide",
                "url": "https://mekoclinic.com/recovery-guide",
                "description": "‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î"
            }
        ]
    },
    "consultation": {
        "videos": [
            {
                "title": "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏°‡πÇ‡∏Å‡∏∞‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å",
                "url": "https://www.youtube.com/watch?v=consult1",
                "description": "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏î‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"
            }
        ],
        "websites": [
            {
                "title": "Book Consultation - Meko Clinic",
                "url": "https://mekoclinic.com/book-consultation",
                "description": "‡∏à‡∏≠‡∏á‡∏ô‡∏±‡∏î‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå"
            }
        ]
    }
}

# Initialize OpenAI client
@st.cache_resource
def init_openai_client():
    # Try multiple sources for API key
    api_key = None
    
    # 1. Try environment variable
    api_key = os.getenv("OPENAI_API_KEY")
    
    # 2. Try Streamlit secrets
    if not api_key:
        try:
            api_key = st.secrets.get("OPENAI_API_KEY", "")
        except:
            pass
    
    # 3. Try .env file (if dotenv is loaded)
    if not api_key:
        try:
            load_dotenv()
            api_key = os.getenv("OPENAI_API_KEY")
        except:
            pass
    
    # 4. Show configuration help if no key found
    if not api_key:
        st.error("‚ö†Ô∏è OpenAI API Key not found!")
        st.markdown("""
        **Please set up your API key in one of these ways:**
        
        1. **Create a `.env` file** in the project root:
        ```
        OPENAI_API_KEY=your-actual-api-key-here
        ```
        
        2. **Set environment variable:**
        ```bash
        export OPENAI_API_KEY=your-actual-api-key-here
        ```
        
        3. **Create `.streamlit/secrets.toml`:**
        ```toml
        OPENAI_API_KEY = "your-actual-api-key-here"
        ```
        
        4. **For Streamlit Cloud:** Add to app secrets in the dashboard
        """)
        
        # Show a text input for temporary testing
        st.markdown("### üîë Temporary API Key Input (for testing)")
        temp_key = st.text_input("Enter your OpenAI API key (temporary):", type="password")
        if temp_key:
            api_key = temp_key
        else:
            st.stop()
    
    # Validate API key format
    if api_key and not api_key.startswith("sk-"):
        st.error("‚ùå Invalid API key format. API key should start with 'sk-'")
        st.stop()
    
    try:
        client = OpenAI(api_key=api_key)
        # Test the connection
        client.models.list()
        return client
    except Exception as e:
        st.error(f"‚ùå Failed to initialize OpenAI client: {str(e)}")
        st.markdown("""
        **Common issues:**
        - API key is invalid or expired
        - Network connection issues
        - OpenAI service is down
        
        **Please check your API key and try again.**
        """)
        st.stop()

# Enhanced language detection
def detect_language(text):
    try:
        text_lower = text.lower()
        
        # Check for native scripts
        if re.search(r'[\u0600-\u06FF]', text):
            urdu_words = ['€å€Å', '⁄©€åÿß', '€Å€í', 'ÿßŸàÿ±', 'ŸÖ€å⁄∫']
            arabic_words = ['Ÿáÿ∞ÿß', 'ŸÅŸä', 'ŸÖŸÜ', 'ÿπŸÑŸâ']
            urdu_count = sum(1 for word in urdu_words if word in text)
            arabic_count = sum(1 for word in arabic_words if word in text)
            return "Urdu" if urdu_count > arabic_count else "Arabic"
        
        if re.search(r'[\u0E00-\u0E7F]', text): return "Thai"
        if re.search(r'[\u0900-\u097F]', text): return "Hindi"
        if re.search(r'[\u4e00-\u9fff]', text): return "Chinese"
        if re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text): return "Japanese"
        if re.search(r'[\uac00-\ud7af]', text): return "Korean"
        if re.search(r'[\u0400-\u04FF]', text): return "Russian"
        
        # Check for Roman scripts
        thai_patterns = [r'\b(chai|mai|krub|krab|arai|yang|ngai|sabai)\b']
        urdu_patterns = [r'\b(kya|hai|yaar|kar|hona|lagta|hota|mera|tera)\b']
        hindi_patterns = [r'\b(kya|hai|kar|hona|mera|tera|hamara|mata)\b']
        
        thai_score = sum(1 for p in thai_patterns if re.search(p, text_lower))
        urdu_score = sum(1 for p in urdu_patterns if re.search(p, text_lower))
        hindi_score = sum(1 for p in hindi_patterns if re.search(p, text_lower))
        
        if thai_score > 0: return "Thai"
        if urdu_score > hindi_score: return "Urdu"
        if hindi_score > 0: return "Hindi"
        
        # Use langdetect for other languages
        detected = detect(text)
        return LANG_DETECT_MAP.get(detected, "English")
        
    except:
        return "English"

# Extract links and videos from HTML content
def extract_links_from_html(html_content):
    """Extract video and website links from HTML content"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    links = {
        "videos": [],
        "websites": []
    }
    
    # Extract video links
    video_elements = soup.find_all(['video', 'iframe'])
    for video in video_elements:
        src = video.get('src', '')
        if src:
            links["videos"].append({
                "title": video.get('title', 'Video'),
                "url": src,
                "description": "Video content from clinic website"
            })
    
    # Extract website links
    website_links = soup.find_all('a', href=True)
    for link in website_links:
        href = link.get('href')
        if href and not href.startswith('#'):
            # Filter for relevant links
            if any(keyword in href.lower() for keyword in ['rhinoplasty', 'nose', 'surgery', 'clinic', 'meko']):
                links["websites"].append({
                    "title": link.get_text(strip=True) or "Related Link",
                    "url": href,
                    "description": "Related information from clinic website"
                })
    
    return links

# Web search functionality using OpenAI's web search
def search_web_for_resources(query, language="English"):
    """Get related resources from predefined database instead of web search"""
    try:
        # Return predefined resources based on query keywords
        query_lower = query.lower()
        
        # Define search keywords and their corresponding resources
        search_keywords = {
            "rhinoplasty": ["rhinoplasty", "nose surgery", "‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å", "‡∏à‡∏°‡∏π‡∏Å"],
            "recovery": ["recovery", "healing", "post-op", "‡∏û‡∏±‡∏Å‡∏ü‡∏∑‡πâ‡∏ô", "‡∏´‡∏≤‡∏¢"],
            "consultation": ["consultation", "appointment", "‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤", "‡∏ô‡∏±‡∏î"],
            "pricing": ["price", "cost", "‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢"],
            "contact": ["contact", "phone", "email", "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠", "‡πÇ‡∏ó‡∏£"]
        }
        
        # Find matching resources
        matching_resources = []
        for category, keywords in search_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                if category in RELATED_RESOURCES:
                    resources = RELATED_RESOURCES[category]
                    if "videos" in resources:
                        matching_resources.extend(resources["videos"][:2])
                    if "websites" in resources:
                        matching_resources.extend(resources["websites"][:2])
        
        # If no specific matches, return general rhinoplasty resources
        if not matching_resources and "rhinoplasty" in RELATED_RESOURCES:
            resources = RELATED_RESOURCES["rhinoplasty"]
            if "videos" in resources:
                matching_resources.extend(resources["videos"][:2])
            if "websites" in resources:
                matching_resources.extend(resources["websites"][:2])
        
        return matching_resources
        
    except Exception as e:
        st.error(f"Resource search error: {str(e)}")
        return []

# Get related resources based on query
def get_related_resources(user_query, language="English"):
    """Get related video and website links based on user query"""
    query_lower = user_query.lower()
    
    # Check predefined resources first
    related_links = []
    
    for category, resources in RELATED_RESOURCES.items():
        if category in query_lower or any(word in query_lower for word in ["rhinoplasty", "nose", "surgery", "‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å", "‡∏à‡∏°‡∏π‡∏Å", "‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î"]):
            if "videos" in resources:
                related_links.extend(resources["videos"][:2])  # Limit to 2 videos
            if "websites" in resources:
                related_links.extend(resources["websites"][:2])  # Limit to 2 websites
    
    # If no predefined resources found, try the updated search function
    if not related_links:
        try:
            search_results = search_web_for_resources(user_query, language)
            if search_results:
                related_links = search_results[:4]  # Limit to 4 results
        except Exception as e:
            # Silently handle any errors to prevent chatbot crashes
            pass
    
    return related_links

# Format related resources for display
def format_related_resources(links, language="English"):
    """Format related resources for display in the response"""
    if not links:
        return ""
    
    if language == "Thai":
        header = "\n\nüì∫ **‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÅ‡∏•‡∏∞‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:**\n"
    else:
        header = "\n\nüì∫ **Related Videos & Links:**\n"
    
    formatted_links = header
    
    for i, link in enumerate(links[:4], 1):  # Limit to 4 links
        title = link.get("title", "Related Resource")
        url = link.get("url", "#")
        description = link.get("description", "")
        
        formatted_links += f"{i}. **{title}**\n"
        formatted_links += f"   üîó [{url}]({url})\n"
        if description:
            formatted_links += f"   üìù {description}\n"
        formatted_links += "\n"
    
    return formatted_links

# Enhanced content processing with structured data extraction
@st.cache_data
def load_and_process_html_content():
    try:
        # Try multiple possible paths for the HTML file
        possible_paths = [
            "meko_clinic_rhinoplasty.html",
            "./meko_clinic_rhinoplasty.html",
            os.path.join(os.getcwd(), "meko_clinic_rhinoplasty.html"),
            os.path.join(os.path.dirname(__file__), "meko_clinic_rhinoplasty.html")
        ]
        
        html_content = None
        used_path = None
        
        for path in possible_paths:
            try:
                if os.path.exists(path):
                    with open(path, "r", encoding="utf-8") as file:
                        html_content = file.read()
                        used_path = path
                        break
            except Exception as e:
                continue
        
        if not html_content:
            # Try to find the file in the current directory
            try:
                current_dir = os.getcwd()
                files = os.listdir(current_dir)
                html_files = [f for f in files if f.endswith('.html')]
                
                if html_files:
                    # Use the first HTML file found
                    html_file_path = os.path.join(current_dir, html_files[0])
                    with open(html_file_path, "r", encoding="utf-8") as file:
                        html_content = file.read()
                        used_path = html_file_path
                        st.info(f"üìÑ Found HTML file: {html_files[0]}")
            except Exception as e:
                pass
        
        if not html_content:
            st.warning("‚ö†Ô∏è HTML file 'meko_clinic_rhinoplasty.html' not found. Using fallback content.")
            st.info("üí° This might be due to deployment environment differences. The app will still work with fallback content.")
            return get_fallback_content(), {}
        
        # Log which path was used (for debugging)
        if used_path:
            st.success(f"‚úÖ Successfully loaded HTML content from: {used_path}")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove unwanted elements
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # Extract structured data
        structured_data = {
            "procedures": [],
            "pricing": [],
            "contact": [],
            "reviews": [],
            "gallery": [],
            "videos": [],
            "doctors": [],
            "locations": []
        }
        
        # Extract text content
        h = html2text.HTML2Text()
        h.ignore_links = True
        h.ignore_images = True
        h.body_width = 0
        text_content = h.handle(str(soup))
        
        # Clean up the text
        text_content = re.sub(r'\n\s*\n', '\n\n', text_content)
        text_content = re.sub(r'\s+', ' ', text_content)
        
        # Extract specific information patterns
        # Pricing
        price_patterns = [
            r'(\d{2,3}(?:,\d{3})*)\s*‡∏ö‡∏≤‡∏ó',
            r'‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏µ‡∏¢‡∏á\s*(\d{2,3}(?:,\d{3})*)\s*‡∏ö‡∏≤‡∏ó',
            r'‡∏£‡∏≤‡∏Ñ‡∏≤\s*(\d{2,3}(?:,\d{3})*)\s*‡∏ö‡∏≤‡∏ó'
        ]
        for pattern in price_patterns:
            matches = re.findall(pattern, text_content)
            structured_data["pricing"].extend(matches)
        
        # Contact information
        contact_patterns = [
            r'\+66\s*2\s*272\s*0022',
            r'@MEKOCLINIC',
            r'mekoclinic\.com'
        ]
        for pattern in contact_patterns:
            matches = re.findall(pattern, text_content)
            structured_data["contact"].extend(matches)
        
        # Procedures
        procedure_keywords = [
            "‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡∏¥‡∏î", "Open Rhinoplasty", "‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å", "nose surgery",
            "‡∏ï‡∏∞‡πÑ‡∏ö‡∏à‡∏°‡∏π‡∏Å", "‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á", "‡πÅ‡∏Å‡πâ‡∏à‡∏°‡∏π‡∏Å", "revision rhinoplasty"
        ]
        for keyword in procedure_keywords:
            if keyword in text_content:
                structured_data["procedures"].append(keyword)
        
        # Extract links from HTML
        extracted_links = extract_links_from_html(html_content)
        structured_data["videos"] = extracted_links["videos"]
        structured_data["websites"] = extracted_links["websites"]
        
        return text_content, structured_data
        
    except Exception as e:
        st.error(f"Error loading HTML content: {str(e)}")
        st.info("üí° Using fallback content. The app will still function normally.")
        return get_fallback_content(), {}

# Fallback content when HTML file is not available
def get_fallback_content():
    return """
    MEKO CLINIC - RHINOPLASTY SPECIALISTS
    
    About Us:
    Meko Clinic is a leading medical facility specializing in rhinoplasty (nose surgery) procedures. 
    We provide comprehensive cosmetic and reconstructive nose surgery services.
    
    Our Services:
    - Primary Rhinoplasty
    - Revision Rhinoplasty 
    - Ethnic Rhinoplasty
    - Functional Rhinoplasty
    - Non-surgical Nose Jobs
    - Consultation Services
    
    Why Choose Meko Clinic:
    - Expert surgeons with years of experience
    - State-of-the-art facilities
    - Personalized treatment plans
    - Comprehensive aftercare support
    - Natural-looking results
    
    Procedure Information:
    Rhinoplasty can address various concerns including:
    - Nose size and shape
    - Nostril size and shape
    - Nasal tip refinement
    - Bridge adjustments
    - Breathing improvements
    
    Recovery Process:
    - Initial healing: 1-2 weeks
    - Return to normal activities: 2-3 weeks
    - Full results visible: 6-12 months
    
    Consultation Process:
    - Initial assessment
    - Digital imaging
    - Surgical planning
    - Pre-operative instructions
    - Follow-up care
    
    Contact Information:
    For consultations and appointments, please contact Meko Clinic directly.
    We offer both in-person and virtual consultations.
    """

# Smart query classification
def classify_query(user_message, clinic_content):
    """Classify if the query is clinic-related or general"""
    clinic_keywords = [
        "rhinoplasty", "nose surgery", "nose job", "‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å", "‡∏à‡∏°‡∏π‡∏Å", "surgery",
        "clinic", "doctor", "procedure", "recovery", "consultation", "price",
        "cost", "appointment", "meko", "‡πÄ‡∏°‡πÇ‡∏Å‡∏∞", "‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î", "‡∏®‡∏±‡∏•‡∏¢‡∏Å‡∏£‡∏£‡∏°"
    ]
    
    user_lower = user_message.lower()
    clinic_score = sum(1 for keyword in clinic_keywords if keyword in user_lower)
    
    # Check if query contains clinic-specific content
    clinic_content_lower = clinic_content.lower()
    content_matches = sum(1 for keyword in clinic_keywords if keyword in clinic_content_lower)
    
    # If query has clinic keywords or matches clinic content, it's clinic-related
    if clinic_score > 0 or content_matches > 0:
        return "clinic_related"
    else:
        return "general"

# Enhanced semantic search
@st.cache_data
def create_semantic_search_index(clinic_content):
    """Create a semantic search index for better content retrieval"""
    try:
        # Split content into chunks
        chunks = re.split(r'\n\n+', clinic_content)
        chunks = [chunk.strip() for chunk in chunks if len(chunk.strip()) > 50]
        
        # Create TF-IDF vectorizer
        vectorizer = TfidfVectorizer(
            stop_words='english',
            ngram_range=(1, 2),
            max_features=1000
        )
        
        # Fit and transform
        tfidf_matrix = vectorizer.fit_transform(chunks)
        
        return {
            'vectorizer': vectorizer,
            'tfidf_matrix': tfidf_matrix,
            'chunks': chunks
        }
    except Exception as e:
        st.error(f"Error creating search index: {str(e)}")
        return None

def semantic_search(query, search_index, top_k=3):
    """Perform semantic search on clinic content"""
    if not search_index:
        return []
    
    try:
        # Transform query
        query_vector = search_index['vectorizer'].transform([query])
        
        # Calculate similarities
        similarities = cosine_similarity(query_vector, search_index['tfidf_matrix'])
        
        # Get top matches
        top_indices = similarities[0].argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[0][idx] > 0.1:  # Minimum similarity threshold
                results.append({
                    'content': search_index['chunks'][idx],
                    'similarity': similarities[0][idx]
                })
        
        return results
    except Exception as e:
        st.error(f"Error in semantic search: {str(e)}")
        return []

# Token counting function
def count_tokens(text):
    """Rough token count estimation (1 token ‚âà 4 characters)"""
    return len(text) // 4

# Smart content selection based on user query with better token management
def select_relevant_content(user_message, clinic_content, search_index=None, max_tokens=1500):
    """Select the most relevant content based on user query with strict token limits"""
    
    # First try semantic search if available
    if search_index:
        semantic_results = semantic_search(user_message, search_index, top_k=1)  # Reduced to 1 result
        if semantic_results:
            selected_content = semantic_results[0]['content']
            if count_tokens(selected_content) <= max_tokens:
                return selected_content
    
    # Fallback to keyword-based selection
    keywords_map = {
        "procedure": ["procedure", "surgery", "operation", "rhinoplasty", "nose job", "plastic surgery", "‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏à‡∏°‡∏π‡∏Å", "‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î"],
        "recovery": ["recovery", "healing", "aftercare", "post-op", "swelling", "bruising", "rest", "‡∏û‡∏±‡∏Å‡∏ü‡∏∑‡πâ‡∏ô", "‡∏´‡∏≤‡∏¢"],
        "cost": ["cost", "price", "fee", "payment", "insurance", "financing", "expensive", "‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢"],
        "consultation": ["consultation", "appointment", "visit", "meet", "doctor", "surgeon", "‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤", "‡∏ô‡∏±‡∏î‡∏´‡∏°‡∏≤‡∏¢"],
        "types": ["types", "kinds", "different", "options", "primary", "revision", "ethnic", "‡πÅ‡∏ö‡∏ö", "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó"],
        "results": ["results", "outcome", "before", "after", "expect", "appearance", "look", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå", "‡∏ú‡∏•‡∏á‡∏≤‡∏ô"]
    }
    
    user_lower = user_message.lower()
    
    # Split content into sections
    sections = clinic_content.split('\n\n')
    scored_sections = []
    
    for section in sections:
        if len(section.strip()) < 50:  # Skip very short sections
            continue
            
        section_lower = section.lower()
        score = 0
        
        # Score based on keyword matches
        for category, keywords in keywords_map.items():
            for keyword in keywords:
                if keyword in user_lower:
                    score += section_lower.count(keyword) * 2
                score += section_lower.count(keyword)
        
        # Boost score for sections with general medical terms
        medical_terms = ["rhinoplasty", "nose", "surgery", "procedure", "clinic", "doctor", "patient", "‡∏à‡∏°‡∏π‡∏Å", "‡πÄ‡∏°‡πÇ‡∏Å‡∏∞"]
        for term in medical_terms:
            score += section_lower.count(term)
        
        scored_sections.append((score, section))
    
    # Sort by relevance and select top sections with strict token limit
    scored_sections.sort(key=lambda x: x[0], reverse=True)
    
    selected_content = ""
    current_tokens = 0
    
    for score, section in scored_sections:
        section_tokens = count_tokens(section)
        if current_tokens + section_tokens <= max_tokens:
            selected_content += section + "\n\n"
            current_tokens += section_tokens
        else:
            # If we can't fit the full section, try to fit a portion
            if current_tokens < max_tokens * 0.8:  # Leave some buffer
                words = section.split()
                partial_section = ""
                for word in words:
                    test_section = partial_section + " " + word if partial_section else word
                    if count_tokens(test_section) <= max_tokens - current_tokens:
                        partial_section = test_section
                    else:
                        break
                if partial_section:
                    selected_content += partial_section + "\n\n"
            break
    
    return selected_content.strip() if selected_content else clinic_content[:max_tokens*3]  # Reduced multiplier

# Enhanced response generation with smart context management and related resources
def generate_response(user_message, detected_language, clinic_content, search_index=None, conversation_history=None):
    try:
        client = init_openai_client()
        
        # Classify query
        query_type = classify_query(user_message, clinic_content)
        
        # Check for quick response templates
        user_lower = user_message.lower()
        for key, responses in QUICK_RESPONSES.items():
            if key in user_lower or any(word in user_lower for word in ["‡∏£‡∏≤‡∏Ñ‡∏≤", "price", "cost", "‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤", "consult", "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠", "contact", "‡∏û‡∏±‡∏Å‡∏ü‡∏∑‡πâ‡∏ô", "recovery"]):
                if detected_language in responses:
                    base_response = responses[detected_language]
                    # Add related resources to quick responses
                    related_links = get_related_resources(user_message, detected_language)
                    if related_links:
                        base_response += format_related_resources(related_links, detected_language)
                    return base_response
        
        lang_info = LANGUAGES.get(detected_language, LANGUAGES["English"])
        native_name = lang_info["native"]
        
        # Select relevant content with reduced token limit
        relevant_content = select_relevant_content(user_message, clinic_content, search_index, max_tokens=1000)
        
        # Build conversation context with strict limits
        context = ""
        if conversation_history and len(conversation_history) > 0:
            # Only use last 2 messages to save tokens
            recent_messages = conversation_history[-2:]
            context_parts = []
            for msg in recent_messages:
                # Truncate long messages
                content = msg['content'][:200] if len(msg['content']) > 200 else msg['content']
                context_parts.append(f"{msg['role']}: {content}")
            context = "\n".join(context_parts)
        
        # Get related resources
        related_links = get_related_resources(user_message, detected_language)
        
        # Create system prompt based on query type with reduced content
        if query_type == "clinic_related":
            system_prompt = f"""You are a medical assistant for Meko Clinic specializing in rhinoplasty.

LANGUAGE: Respond in {detected_language} ({native_name} script)

ROLE: Provide expert information about rhinoplasty procedures, recovery, consultations, and clinic services.

CLINIC INFO:
{relevant_content}

CONVERSATION CONTEXT:
{context}

GUIDELINES:
- Be professional, empathetic, and detailed
- Use the provided clinic information as primary source
- Recommend consulting doctors for medical advice
- Include specific details about procedures, timelines, and care
- Maintain medical confidentiality and ethics
- If information is not in clinic data, clearly state it's general information"""
        else:
            system_prompt = f"""You are a helpful AI assistant.

LANGUAGE: Respond in {detected_language} ({native_name} script)

ROLE: Provide helpful information on general topics.

GUIDELINES:
- Be informative and helpful
- Provide accurate information
- Be conversational and friendly
- If medical advice is requested, recommend consulting healthcare professionals"""

        # Calculate remaining tokens for response with strict limits
        system_tokens = count_tokens(system_prompt)
        user_tokens = count_tokens(user_message)
        max_response_tokens = min(800, 7000 - system_tokens - user_tokens - 500)  # Reduced limits
        
        # Ensure we don't exceed token limits
        if max_response_tokens < 100:
            max_response_tokens = 100
        
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=max_response_tokens,
            top_p=0.9,
            frequency_penalty=0.1,
            presence_penalty=0.1
        )
        
        base_response = response.choices[0].message.content
        
        # Add related resources to the response if not already included (with token check)
        if related_links and "üì∫" not in base_response and "Related" not in base_response:
            resources_text = format_related_resources(related_links, detected_language)
            if count_tokens(base_response + resources_text) < 1500:  # Check total response length
                base_response += resources_text
        
        return base_response
        
    except Exception as e:
        error_messages = {
            "Thai": f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {str(e)}",
            "Urdu": f"‚ùå ÿ¨Ÿàÿßÿ® ÿ®ŸÜÿßÿ™€í ŸàŸÇÿ™ ÿÆÿ±ÿßÿ®€å: {str(e)}",
            "Arabic": f"‚ùå ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ∑ŸÑÿ®: {str(e)}",
            "Hindi": f"‚ùå ‡§â‡§§‡•ç‡§§‡§∞ ‡§¨‡§®‡§æ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {str(e)}",
            "Spanish": f"‚ùå Error al procesar la solicitud: {str(e)}",
            "French": f"‚ùå Erreur lors du traitement: {str(e)}",
            "German": f"‚ùå Fehler bei der Verarbeitung: {str(e)}",
            "Chinese": f"‚ùå Â§ÑÁêÜËØ∑Ê±ÇÊó∂Âá∫Èîô: {str(e)}",
            "Japanese": f"‚ùå Âá¶ÁêÜ„Ç®„É©„Éº: {str(e)}"
        }
        return error_messages.get(detected_language, f"‚ùå Error generating response: {str(e)}")

# Enhanced transcription with better error handling
def transcribe_audio(audio_bytes):
    try:
        client = init_openai_client()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_file.write(audio_bytes)
            tmp_file_path = tmp_file.name
        
        with open(tmp_file_path, "rb") as audio_file:
            transcription = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                response_format="text",
                language=None,
                temperature=0.2
            )
        
        os.unlink(tmp_file_path)
        return transcription.strip()
    except Exception as e:
        st.error(f"‚ùå Transcription error: {str(e)}")
        return None

# Enhanced speech generation with better quality
def generate_speech(text, language):
    try:
        client = init_openai_client()
        voice = LANGUAGES.get(language, LANGUAGES["English"])["voice"]
        
        # Truncate text if too long, but try to cut at sentence boundaries
        max_length = 2000
        if len(text) > max_length:
            truncated = text[:max_length]
            last_sentence = truncated.rfind('.')
            if last_sentence > max_length * 0.8:
                text = truncated[:last_sentence + 1]
            else:
                text = truncated + "..."
        
        response = client.audio.speech.create(
            model="tts-1-hd",
            voice=voice,
            input=text,
            speed=0.9
        )
        return response.content
    except Exception as e:
        st.error(f"‚ùå Speech generation error: {str(e)}")
        return None

# Create audio player with better styling
def create_audio_player(audio_content):
    audio_base64 = base64.b64encode(audio_content).decode()
    return f"""
    <div style="margin: 10px 0;">
        <audio controls style="width: 100%; max-width: 400px;">
            <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
            Your browser does not support the audio element.
        </audio>
    </div>
    """

# Export conversation
def export_conversation(messages, format="json"):
    if format == "json":
        return json.dumps(messages, indent=2, ensure_ascii=False)
    elif format == "txt":
        text = "Meko Clinic Chatbot Conversation\n"
        text += "=" * 40 + "\n\n"
        for msg in messages:
            text += f"{msg['role'].upper()}: {msg['content']}\n\n"
        return text
    elif format == "csv":
        df = pd.DataFrame(messages)
        return df.to_csv(index=False)

# Function to manage conversation history and prevent token overflow
def manage_conversation_history(messages, max_messages=10):
    """Keep conversation history manageable to prevent token overflow"""
    if len(messages) > max_messages:
        # Keep the first message (greeting) and the last max_messages-1 messages
        return [messages[0]] + messages[-(max_messages-1):]
    return messages

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "clinic_content" not in st.session_state:
    st.session_state.clinic_content, st.session_state.structured_data = load_and_process_html_content()
if "voice_enabled" not in st.session_state:
    st.session_state.voice_enabled = True
if "last_audio_hash" not in st.session_state:
    st.session_state.last_audio_hash = None
if "conversation_context" not in st.session_state:
    st.session_state.conversation_context = []
if "search_index" not in st.session_state:
    st.session_state.search_index = create_semantic_search_index(st.session_state.clinic_content)
if "analytics" not in st.session_state:
    st.session_state.analytics = {
        "total_queries": 0,
        "clinic_queries": 0,
        "general_queries": 0,
        "voice_queries": 0,
        "text_queries": 0,
        "languages_used": defaultdict(int),
        "response_times": []
    }

# Manage conversation history to prevent token overflow
st.session_state.messages = manage_conversation_history(st.session_state.messages)

# Enhanced sidebar with more options
with st.sidebar:
    st.title("üè• Meko Clinic")
    st.markdown("### Advanced Rhinoplasty Chatbot")
    
    # Display content status
    content_length = len(st.session_state.clinic_content)
    st.info(f"üìÑ Clinic content loaded: {content_length:,} characters")
    
    # Quick actions
    st.markdown("### ‚ö° Quick Actions")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üí∞ Pricing", help="Get pricing information"):
            st.session_state.quick_action = "pricing"
    
    with col2:
        if st.button("üìã Consultation", help="Book consultation"):
            st.session_state.quick_action = "consultation"
    
    col3, col4 = st.columns(2)
    with col3:
        if st.button("ü©π Recovery", help="Recovery information"):
            st.session_state.quick_action = "recovery"
    
    with col4:
        if st.button("üìû Contact", help="Contact information"):
            st.session_state.quick_action = "contact"
    
    # Voice settings
    voice_enabled = st.toggle("üéôÔ∏è Enable Voice Features", value=st.session_state.voice_enabled)
    st.session_state.voice_enabled = voice_enabled
    
    # Language selection
    selected_language = st.selectbox(
        "üåê Language Override",
        options=["Auto-detect"] + list(LANGUAGES.keys()),
        index=0,
        help="Select a specific language or use auto-detection"
    )
    
    # Model settings
    st.markdown("### ‚öôÔ∏è AI Model Settings")
    st.info("Using GPT-4 for high-quality responses")
    st.info("Using Whisper for accurate transcription")
    st.info("Using TTS-1-HD for clear speech")
    
    # Export options
    st.markdown("### üì§ Export Options")
    export_format = st.selectbox("Export format", ["json", "txt", "csv"])
    if st.button("üì• Export Conversation"):
        if st.session_state.messages:
            export_data = export_conversation(st.session_state.messages, export_format)
            st.download_button(
                label="Download",
                data=export_data,
                file_name=f"meko_chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{export_format}",
                mime="text/plain"
            )
    
    # Clear chat
    if st.button("üóëÔ∏è Clear Chat History"):
        st.session_state.messages = []
        st.session_state.last_audio_hash = None
        st.session_state.conversation_context = []
        st.rerun()
    
    # Analytics
    if st.session_state.analytics["total_queries"] > 0:
        st.markdown("### üìä Analytics")
        st.metric("Total Queries", st.session_state.analytics["total_queries"])
        st.metric("Clinic Queries", st.session_state.analytics["clinic_queries"])
        st.metric("Voice Queries", st.session_state.analytics["voice_queries"])
        
        if st.session_state.analytics["response_times"]:
            avg_time = np.mean(st.session_state.analytics["response_times"])
            st.metric("Avg Response Time", f"{avg_time:.1f}s")
    
    # Conversation stats
    if st.session_state.messages:
        total_messages = len(st.session_state.messages)
        user_messages = len([m for m in st.session_state.messages if m["role"] == "user"])
        st.markdown(f"**Messages:** {total_messages} ({user_messages} from you)")
    
    # Supported languages
    st.markdown("### üåç Supported Languages")
    st.markdown("""
    **Full Support:** English, Spanish, French, German, Italian, Portuguese, Russian, Chinese, Japanese, Korean, Arabic, Hindi, Urdu, Turkish, Thai
    
    **Features:** Voice input/output, native script support, cultural context awareness
    """)
    

    # Tips
    st.markdown("### üí° Tips for Better Results")
    st.markdown("""
    - Ask specific questions about procedures
    - Mention your concerns or goals
    - Ask about recovery times
    - Inquire about consultation process
    - Use voice input for natural conversation
    - Use quick action buttons for common questions
    """)

# Main interface with enhanced styling
st.title("üí¨ Meko Clinic Rhinoplasty Assistant")
st.markdown("*Ask me anything about rhinoplasty procedures, recovery, consultations, and our services*")

# Handle quick actions
if hasattr(st.session_state, 'quick_action') and st.session_state.quick_action:
    action = st.session_state.quick_action
    detected_lang = selected_language if selected_language != "Auto-detect" else "English"
    
    if action in QUICK_RESPONSES and detected_lang in QUICK_RESPONSES[action]:
        response = QUICK_RESPONSES[action][detected_lang]
        
        # Add to conversation
        st.session_state.messages.append({
            "role": "user",
            "content": f"Quick action: {action}",
            "detected_language": detected_lang,
            "input_type": "quick_action"
        })
        
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "language": detected_lang,
            "has_audio": True
        })
        
        # Update analytics
        st.session_state.analytics["total_queries"] += 1
        st.session_state.analytics["clinic_queries"] += 1
        
        del st.session_state.quick_action
        st.rerun()

# Enhanced chat history display with auto-scroll
if st.session_state.messages:
    st.markdown("### üí¨ Conversation History")
    
    # Create a container for chat messages
    chat_container = st.container()
    
    with chat_container:
        for i, msg in enumerate(st.session_state.messages):
            if msg["role"] == "user":
                lang = msg.get('detected_language', 'Unknown')
                input_type = msg.get('input_type', 'text')
                icon = "üé§" if input_type == "voice" else "üí¨" if input_type == "text" else "‚ö°"
                
                with st.chat_message("user"):
                    st.markdown(f"**{icon} ({lang}):** {msg['content']}")
            else:
                with st.chat_message("assistant"):
                    st.markdown(msg['content'])
                    
                    # Enhanced voice playback for assistant messages
                    if st.session_state.voice_enabled and msg.get('language') and msg.get('has_audio'):
                        col1, col2 = st.columns([1, 4])
                        
                        with col1:
                            if st.button(f"üîä Play", key=f"play_{i}", help="Generate and play audio response"):
                                with st.spinner("üéµ Generating high-quality audio..."):
                                    speech_bytes = generate_speech(msg['content'], msg['language'])
                                    if speech_bytes:
                                        st.markdown(create_audio_player(speech_bytes), unsafe_allow_html=True)
                                    else:
                                        st.error("‚ùå Failed to generate audio")
                        
                        with col2:
                            st.caption(f"Language: {msg.get('language', 'Unknown')}")

# Auto-scroll to bottom when new messages are added
if st.session_state.messages:
    # Add a spacer to push content to bottom
    st.markdown("<div style='height: 100px;'></div>", unsafe_allow_html=True)
    
    # Add auto-scroll JavaScript
    st.markdown("""
    <script>
        // Function to scroll to bottom
        function scrollToBottom() {
            window.scrollTo({
                top: document.body.scrollHeight,
                behavior: 'smooth'
            });
        }
        
        // Scroll to bottom when page loads
        window.addEventListener('load', scrollToBottom);
        
        // Scroll to bottom when new content is added
        const observer = new MutationObserver(function(mutations) {
            mutations.forEach(function(mutation) {
                if (mutation.type === 'childList') {
                    setTimeout(scrollToBottom, 100);
                }
            });
        });
        
        observer.observe(document.body, {
            childList: true,
            subtree: true
        });
        
        // Also scroll when Streamlit reruns
        if (window.parent !== window) {
            window.parent.postMessage({
                type: 'streamlit:setComponentValue',
                value: 'scroll'
            }, '*');
        }
    </script>
    """, unsafe_allow_html=True)

# Input section at the bottom
st.markdown("---")
st.markdown("### üí¨ Ask Your Question")

# Voice input section (moved to bottom)
if st.session_state.voice_enabled:
    st.markdown("#### üéôÔ∏è Voice Input")
    col1, col2 = st.columns([3, 1])
    
    with col1:
        audio_bytes = audio_recorder(
            text="üé§ Click to record your question",
            recording_color="#e74c3c",
            neutral_color="#3498db",
            key="voice_input"
        )
    
    with col2:
        if audio_bytes:
            st.success("‚úÖ Audio recorded")
    
    # Process audio with better handling
    if audio_bytes:
        audio_hash = hashlib.md5(audio_bytes).hexdigest()
        
        if audio_hash != st.session_state.last_audio_hash:
            st.session_state.last_audio_hash = audio_hash
            
            with st.spinner("üéß Transcribing your voice..."):
                transcription = transcribe_audio(audio_bytes)
            
            if transcription:
                st.success(f"üé§ **Transcribed:** {transcription}")
                
                # Process voice input
                detected_language = selected_language if selected_language != "Auto-detect" else detect_language(transcription)
                
                # Add user message
                st.session_state.messages.append({
                    "role": "user",
                    "content": transcription,
                    "detected_language": detected_language,
                    "input_type": "voice"
                })
                
                # Update analytics
                st.session_state.analytics["total_queries"] += 1
                st.session_state.analytics["voice_queries"] += 1
                st.session_state.analytics["languages_used"][detected_language] += 1
                
                # Generate response
                start_time = time.time()
                with st.spinner("ü§ñ Generating detailed response..."):
                    reply = generate_response(
                        transcription, 
                        detected_language, 
                        st.session_state.clinic_content,
                        st.session_state.search_index,
                        st.session_state.messages
                    )
                
                response_time = time.time() - start_time
                st.session_state.analytics["response_times"].append(response_time)
                
                # Classify query for analytics
                query_type = classify_query(transcription, st.session_state.clinic_content)
                if query_type == "clinic_related":
                    st.session_state.analytics["clinic_queries"] += 1
                else:
                    st.session_state.analytics["general_queries"] += 1
                
                # Add assistant message
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": reply,
                    "language": detected_language,
                    "has_audio": True
                })
                
                st.rerun()

# Text input section (at the very bottom)
st.markdown("#### üí¨ Text Input")
if prompt := st.chat_input("üí≠ Ask about rhinoplasty procedures, recovery, costs, or consultations..."):
    detected_language = selected_language if selected_language != "Auto-detect" else detect_language(prompt)
    
    # Add user message
    st.session_state.messages.append({
        "role": "user",
        "content": prompt,
        "detected_language": detected_language,
        "input_type": "text"
    })
    
    # Update analytics
    st.session_state.analytics["total_queries"] += 1
    st.session_state.analytics["text_queries"] += 1
    st.session_state.analytics["languages_used"][detected_language] += 1
    
    # Generate response
    start_time = time.time()
    with st.spinner("ü§ñ Generating comprehensive response..."):
        reply = generate_response(
            prompt, 
            detected_language, 
            st.session_state.clinic_content,
            st.session_state.search_index,
            st.session_state.messages
        )
    
    response_time = time.time() - start_time
    st.session_state.analytics["response_times"].append(response_time)
    
    # Classify query for analytics
    query_type = classify_query(prompt, st.session_state.clinic_content)
    if query_type == "clinic_related":
        st.session_state.analytics["clinic_queries"] += 1
    else:
        st.session_state.analytics["general_queries"] += 1
    
    # Add assistant message
    st.session_state.messages.append({
        "role": "assistant",
        "content": reply,
        "language": detected_language,
        "has_audio": True
    })
    
    st.rerun()

# Footer with additional information
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; padding: 20px;'>
    <p>üè• <strong>Meko Clinic Rhinoplasty Assistant</strong></p>
    <p>Powered by GPT-4 ‚Ä¢ Multilingual Support ‚Ä¢ Voice Enabled ‚Ä¢ Smart Search</p>
    <p><em>For medical consultations, please contact Meko Clinic directly</em></p>
</div>
""", unsafe_allow_html=True)